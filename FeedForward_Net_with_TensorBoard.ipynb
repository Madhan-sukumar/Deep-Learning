{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvuzkijsuUCmb6WA4GK9+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhan-sukumar/Deep-Learning/blob/main/FeedForward_Net_with_TensorBoard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **torch** - torch is the main module that holds all the things you need for Tensor computation.\n",
        " - **torch.nn and torch.nn.functional** - The torch.nn module provides many classes and functions to build neural networks. Can think of it as the fundamental building blocks of neural networks: models, all kinds of layers, activation functions, parameter classes, etc. It allows us to build the model like putting some LEGO set together.\n",
        "\n",
        " - **torch.optim** - torch.optim offers all the optimizers like SGD, ADAM, etc., so don’t have to write it from scratch.\n",
        "\n",
        " - **torchvision** - torchvision contains a lot of popular datasets, model architectures, and common image transformations for computer vision. We get our MNIST dataset from it and also use its transforms.\n",
        "\n",
        "- **SummaryWriter (Tensor Board)** - SummaryWriter enables PyTorch to generate the report for Tensor Board. We’ll use Tensor Board to look at our training data, compare results and gain intuition. \n",
        "\n"
      ],
      "metadata": {
        "id": "MMwDmg-AgRmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About the Dataset:**\n",
        "\n",
        "The MNIST dataset, also known as the Modified National Institute of Standards and Technology dataset, consists of 60,000 small square 28×28 grayscale images of handwritten digits between 0 to 9 divided into ten different classes. This dataset is mainly used for text classification using deep learning models.\n",
        "\n",
        "To create a Feed-Forward classification model on the MNIST dataset,\n",
        "\n",
        " 1. Use DataLoader module from Pytorch to load our dataset and Transform It\n",
        " 2. We will implement Neural Net, with input, hidden & output Layer\n",
        " 3. Apply Activation Functions \n",
        " 4. Set up the Loss & Optimizer and implement a Training Loop that can use batch training\n",
        " 5. Finally, evaluate the model and calculate our accuracy. \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "f1BBZlS8C14k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQNYGhMOtfN",
        "outputId": "12b7bcfc-901a-4020-fac8-d4663bad63b6"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.11.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULh6ZkmWRbzZ",
        "outputId": "52246dbd-3e86-44e3-b1d8-974f046d20b7"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (2.12.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.27.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (3.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (1.53.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard) (2.2.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard) (6.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "pBAyXaGZxCox"
      },
      "outputs": [],
      "source": [
        "# imorting dependencies\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for tensorboard\n",
        "#to log the image for the consumption of tensor board using writer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "eIj7HzIVErZr"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "F__jkgNDAaP-"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first define our hyperparameters for our neural network; we are setting our input size to 784 as we know that our dataset contains images of the size 28×28 and flatten this into a one-dimensional array. "
      ],
      "metadata": {
        "id": "aGI5fusN7I6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters tuning\n",
        "input_size = 784 #28x28px input size \n",
        "hidden_size = 100\n",
        "num_classess = 10 # output size or number of classes from 0 to 9\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "ybOxIH8YyRcj"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset from MNIST\n",
        "#converting the data to tensor using transform\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',train=True,\n",
        "                                           transform = transforms.ToTensor(),download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',train=False,\n",
        "                                           transform = transforms.ToTensor())\n"
      ],
      "metadata": {
        "id": "j5po7QTOzlbV"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loaders\n",
        "train_loaders = DataLoader(dataset= train_dataset, batch_size = batch_size,shuffle = True)\n",
        "test_loaders = DataLoader(dataset= test_dataset, batch_size = batch_size,shuffle = False)"
      ],
      "metadata": {
        "id": "IA6Xh_Qj0PwK"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to see \n",
        "examples= iter(train_loaders)\n",
        "samples, labels = next(examples)\n",
        "print(samples.shape,labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLn4ru2x1dVz",
        "outputId": "142eb3b0-624d-48f9-a97d-2e096a480cb8"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows, we having 100 batches of images and the colour channel is 1, its size is 28x28"
      ],
      "metadata": {
        "id": "-MtLM8jy1-4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing the sample data\n",
        "# for i in range(6):\n",
        "#   plt.subplot(2,3,i+1)\n",
        "#   plt.imshow(samples[i][0],cmap='gray')\n",
        "# # plt.show()"
      ],
      "metadata": {
        "id": "TuZR_Nr01qV_"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a image grid to show them on the tensorboard\n",
        "img_grid = torchvision.utils.make_grid(samples)\n",
        "#write image data to tensorboard log dir\n",
        "writer = SummaryWriter('runs/minst')\n",
        "writer.add_image('mnist_images',img_grid)\n"
      ],
      "metadata": {
        "id": "Clxb8qAzGdvE"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FULLY CONNECTED NN"
      ],
      "metadata": {
        "id": "nLl3V4Ss3mt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Fully connected neural network with one hidden layer. We will not be not using the Softmax function here as cross-entropy loss implemented further will apply it automatically. "
      ],
      "metadata": {
        "id": "LkjNZmOc3p2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,num_classess):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    #creatin linear layer\n",
        "    self.l1 = nn.Linear(input_size,hidden_size) \n",
        "    #applying activation function\n",
        "    self.relu = nn.ReLU()\n",
        "    #linear layer at end\n",
        "    self.l2 = nn.Linear(hidden_size,num_classess)\n",
        "\n",
        "  #defining forward, x will get the sample as input\n",
        "  def forward(self,x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "model =NeuralNet(input_size,hidden_size,num_classess)"
      ],
      "metadata": {
        "id": "ESDdAIQh3o7F"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOSS FUNCTIONS AND OPTIMISER "
      ],
      "metadata": {
        "id": "mOQ_RQev9kag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr= learning_rate)"
      ],
      "metadata": {
        "id": "Rc2Ub59-9LHx"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer.add_graph(model, samples.reshape(-1, 28*28).to(device))"
      ],
      "metadata": {
        "id": "CiTjiFucX0yz"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING LOOP"
      ],
      "metadata": {
        "id": "2fWTTaCR-sTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "running_loss = 0.0\n",
        "running_correct = 0\n",
        "n_total_steps = len(train_loaders)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loaders):  # i -index\n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784 as 1d]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device) #pushing the images to GPU to get GPU support\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        running_correct += (predicted == labels).sum().item()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRCygQMn-rn0",
        "outputId": "e90bdf83-9cad-47a4-cdef-368d781f0ea6"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [100/600], Loss: 0.3877\n",
            "Epoch [1/2], Step [100/600], Loss: 0.3877\n",
            "Epoch [1/2], Step [200/600], Loss: 0.3399\n",
            "Epoch [1/2], Step [200/600], Loss: 0.3399\n",
            "Epoch [1/2], Step [300/600], Loss: 0.1683\n",
            "Epoch [1/2], Step [300/600], Loss: 0.1683\n",
            "Epoch [1/2], Step [400/600], Loss: 0.3643\n",
            "Epoch [1/2], Step [400/600], Loss: 0.3643\n",
            "Epoch [1/2], Step [500/600], Loss: 0.3567\n",
            "Epoch [1/2], Step [500/600], Loss: 0.3567\n",
            "Epoch [1/2], Step [600/600], Loss: 0.2039\n",
            "Epoch [1/2], Step [600/600], Loss: 0.2039\n",
            "Epoch [2/2], Step [100/600], Loss: 0.1448\n",
            "Epoch [2/2], Step [100/600], Loss: 0.1448\n",
            "Epoch [2/2], Step [200/600], Loss: 0.2138\n",
            "Epoch [2/2], Step [200/600], Loss: 0.2138\n",
            "Epoch [2/2], Step [300/600], Loss: 0.1935\n",
            "Epoch [2/2], Step [300/600], Loss: 0.1935\n",
            "Epoch [2/2], Step [400/600], Loss: 0.1577\n",
            "Epoch [2/2], Step [400/600], Loss: 0.1577\n",
            "Epoch [2/2], Step [500/600], Loss: 0.1361\n",
            "Epoch [2/2], Step [500/600], Loss: 0.1361\n",
            "Epoch [2/2], Step [600/600], Loss: 0.0520\n",
            "Epoch [2/2], Step [600/600], Loss: 0.0520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
        "running_accuracy = running_correct / 100 / predicted.size(0)\n",
        "writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
        "running_correct = 0\n",
        "running_loss = 0.0"
      ],
      "metadata": {
        "id": "uNbVQSO3YDYf"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "7H1bRvvLB1a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loaders:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        #prediction\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1) # 1 to return index 1\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_TIdDuxAm6u",
        "outputId": "ae2dddf1-0e38-4b46-850d-cb4f6c5031dd"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 95.47 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, predicted = torch.max(outputs.data, 1)\n",
        "running_correct += (predicted == labels).sum().item()\n",
        "if (i+1) % 100 == 0:\n",
        "  print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4mlMeA7YkYt",
        "outputId": "5e981bd9-ea70-4038-bfd8-4070bd3fada9"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Step [600/600], Loss: 0.0520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the tensorboard\n",
        "# %load_ext tensorboard"
      ],
      "metadata": {
        "id": "rvt4amgHAlo4"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Tensorboard to see what's written\n",
        "# %tensorboard --logdir ./runs/ --port=6005\n"
      ],
      "metadata": {
        "id": "rc00vgnLZvXX"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GH1TT8bQaDsR"
      },
      "execution_count": 222,
      "outputs": []
    }
  ]
}