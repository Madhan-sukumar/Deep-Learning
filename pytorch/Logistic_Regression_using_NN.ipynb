{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1G6X0LffMVXzYnry9cyHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhan-sukumar/Deep-Learning/blob/main/pytorch/Logistic_Regression_using_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating and training a neural network involves the following essential steps:\n",
        "\n",
        "1. Define the architecture\n",
        "2. Forward propagate on the architecture using input data\n",
        "3. Calculate the loss\n",
        "4. Backpropagate to calculate the gradient for each weight\n",
        "5. Update the weights using a learning rate\n",
        "\n",
        "Steps include:\n",
        "\n",
        "1. Design model (input size, output size, forward pass)\n",
        "2. construct loss and optimizer\n",
        "3. Training loop \n",
        "      - forward pass: compute prediction and loss\n",
        "      - bakcward pass:  to initiate back propagation to calculate the gradients\n",
        "      - update the weights and iterate "
      ],
      "metadata": {
        "id": "Nkk_owhcXWN2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjnKnp7EXT4o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREPARE THE DATASET"
      ],
      "metadata": {
        "id": "Ict7m0vQY364"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "bc = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "FySwYlBrX5k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the keys\n",
        "print(bc.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWaEUXcwZmd_",
        "outputId": "43e568f7-f6d1-462f-ea37-594af5b3b4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the description of the dataset\n",
        "print(bc.DESCR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y1te2amZVcN",
        "outputId": "8f876095-20c2-4f99-b973-a132f767cebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry\n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        worst/largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
            "        10 is Radius SE, field 20 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the features\n",
        "bc.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyzmziUhZ-mP",
        "outputId": "7ef659d2-52e5-4c28-ee78-b1f0fe7c1f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bc.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZZNJ4_bZba4",
        "outputId": "c1463c70-ffae-4eb6-eac0-93a611632411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
              "        1.189e-01],\n",
              "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
              "        8.902e-02],\n",
              "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
              "        8.758e-02],\n",
              "       ...,\n",
              "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
              "        7.820e-02],\n",
              "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
              "        1.240e-01],\n",
              "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
              "        7.039e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bc.target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6S6mYw3Z16B",
        "outputId": "6ac8aad8-ed3f-4f1e-a9d4-b636a7d5c5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the input data and target\n",
        "x,y = bc.data, bc.target\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmMC0ugJZ3OQ",
        "outputId": "99d78ed4-cc8d-4ed2-b391-992747572250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30)\n",
            "(569,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the rows and columns of X as samples and features\n",
        "n_samples, n_features = x.shape\n",
        "print(n_samples)\n",
        "print(n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdbMEigFavxo",
        "outputId": "4655998b-2ebc-461b-e5da-46d5d5991536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLITTING THE DATA"
      ],
      "metadata": {
        "id": "p8hmYqqpbWvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test, y_train,y_test = train_test_split(x,y,test_size =0.2, random_state = 40)"
      ],
      "metadata": {
        "id": "1H510el6bDCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # STANDARDISING THE DATA"
      ],
      "metadata": {
        "id": "90d-rko0b55l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalar = StandardScaler()\n",
        "X_train = scalar.fit_transform(X_train)\n",
        "X_test = scalar.transform(X_test)"
      ],
      "metadata": {
        "id": "KPs-QrKEbvoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting array to tensor"
      ],
      "metadata": {
        "id": "QwVs-hvPeQTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the numpy array is in double or float64, so we converting it into tensor and float32\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "print(y_train.size())\n",
        "print(y_test.size())\n",
        "\n",
        "#reshaping y tensor\n",
        "y_train = y_train.view(-1,1)\n",
        "y_test = y_test.view(-1,1)\n",
        "print(y_train.size())\n",
        "print(y_test.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3SbKAA3cQPq",
        "outputId": "20ea5e40-475b-4977-b50d-80f91bfd0884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([455])\n",
            "torch.Size([114])\n",
            "torch.Size([455, 1])\n",
            "torch.Size([114, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DESIGNING THE MODEL"
      ],
      "metadata": {
        "id": "FOMGXcMxjdxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the model \n",
        "#here model is linear combination with sigmoid function to create a logistic regression\n",
        "#f = wx +b  with sigmoid function\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  #applying linear layer\n",
        "  def __init__(self,n_input_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    #having only one layer, a builit in linear function\n",
        "    #gets input size and output size or target as 1\n",
        "    self.linear = nn.Linear(n_input_features,1)\n",
        "\n",
        "  #applying sigmoid function\n",
        "  def forward(self,x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n"
      ],
      "metadata": {
        "id": "V_PHrnWtemZm",
        "outputId": "9bcca153-8967-423f-832f-572ccd9e5889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f7c03485a563>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#f = wx +b  with sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;31m#applying linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_input_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting the model\n",
        "model = LogisticRegression(n_features)"
      ],
      "metadata": {
        "id": "Ji5mUSZyjgMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOSS AND OPTIMISER"
      ],
      "metadata": {
        "id": "K1AG-qGKogtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning Binary Cross Entropy Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "##optimiser as stochastic gradient descent\n",
        "optimiser = torch.optim.SGD(model.parameters(),lr =0.01)"
      ],
      "metadata": {
        "id": "TMIYDj6dojsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING LOOP"
      ],
      "metadata": {
        "id": "1NKaCaUZpvNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs =300\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  #forward pass\n",
        "  y_predicted = model(X_train)\n",
        "\n",
        "  #compute loss\n",
        "  loss= criterion(y_predicted, y_train)\n",
        "\n",
        "  #backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  #updating the weigts\n",
        "  optimiser.step()\n",
        "\n",
        "  #empty the gradients\n",
        "  optimiser.zero_grad()\n",
        "  \n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f\"epoch: {epoch+1}, loss: {loss.item():.4f}\") \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfR9Z5WApDi_",
        "outputId": "46bbc50d-4c28-48cc-9a75-204f13757d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss: 0.2465\n",
            "epoch: 20, loss: 0.2364\n",
            "epoch: 30, loss: 0.2275\n",
            "epoch: 40, loss: 0.2197\n",
            "epoch: 50, loss: 0.2128\n",
            "epoch: 60, loss: 0.2066\n",
            "epoch: 70, loss: 0.2010\n",
            "epoch: 80, loss: 0.1958\n",
            "epoch: 90, loss: 0.1911\n",
            "epoch: 100, loss: 0.1868\n",
            "epoch: 110, loss: 0.1829\n",
            "epoch: 120, loss: 0.1792\n",
            "epoch: 130, loss: 0.1757\n",
            "epoch: 140, loss: 0.1725\n",
            "epoch: 150, loss: 0.1695\n",
            "epoch: 160, loss: 0.1667\n",
            "epoch: 170, loss: 0.1641\n",
            "epoch: 180, loss: 0.1616\n",
            "epoch: 190, loss: 0.1592\n",
            "epoch: 200, loss: 0.1570\n",
            "epoch: 210, loss: 0.1549\n",
            "epoch: 220, loss: 0.1529\n",
            "epoch: 230, loss: 0.1510\n",
            "epoch: 240, loss: 0.1492\n",
            "epoch: 250, loss: 0.1475\n",
            "epoch: 260, loss: 0.1458\n",
            "epoch: 270, loss: 0.1442\n",
            "epoch: 280, loss: 0.1427\n",
            "epoch: 290, loss: 0.1413\n",
            "epoch: 300, loss: 0.1399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "QQFuLTD9t1Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before staring predicting, we need to stop the model from tracking the history and forming the backward graph, it set to requires_grad to False and cut it from the computational graph.\n",
        "#if we dont use no_grad() it will become a part of computational graph and calculate gradients\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "\n",
        "  #to print the accuracy\n",
        "  acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])\n",
        "  print(f'accuracy: {acc}:.4f')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-T2nEAEt2o5",
        "outputId": "03f7dd07-5c7d-47a1-8629-232880bb19d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9649122953414917:.4f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7H9EdoV5wW-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}